# BENCHY プロジェクト概要

## プロジェクトの概要

BENCHYは、様々なLLM（大規模言語モデル）のパフォーマンス、価格、速度を実際に体感しながら比較できるライブベンチマークツールです。ユーザーは特定のユースケースにおいて、複数のLLMを並行して実行し、リアルタイムで結果を比較できます。

## 主要な機能

### 1. Thought Bench（思考ベンチ）
- **目的**：複数の推論モデルの思考プロセスと応答を並行して分析
- **対応モデル**：
  - Anthropic Claude 4.0 Sonnet/Opus
  - OpenAI o4-mini/o3
  - Anthropic Claude 3.7 Sonnet
  - Gemini 2.5 Flash/Pro
  - Ollama（Qwen3、Gemma3、Devstral等）
- **特徴**：各モデルの推論過程を可視化し、比較可能

### 2. Iso Speed Bench（等速ベンチ）
- **目的**：統一された設定ファイルベースのマルチLLMプロバイダーベンチマーク
- **評価方式**：はい/いいえの二択評価
- **用途**：高品質な洞察と反復的な改善のための詳細な性能分析

### 3. Long Tool Calling（長いツール呼び出し）
- **目的**：15以上の連続したツール/関数呼び出しに最適なLLMとテクニックの理解
- **対象**：複雑なワークフローや多段階処理を必要とするタスク

### 4. Multi Autocomplete（マルチオートコンプリート）
- **目的**：Claude 3.5 HaikuとGPT-4oの予測出力機能の比較
- **比較対象**：既存モデルとの性能差分析

## 技術スタック

### フロントエンド
- **フレームワーク**：Vue.js 3
- **ビルドツール**：Vite
- **スタイリング**：UnoCSS
- **状態管理**：Pinia Stores
- **UI コンポーネント**：AG-Grid、CodeMirror

### バックエンド
- **言語**：Python
- **フレームワーク**：FastAPI
- **パッケージ管理**：uv
- **対応LLMプロバイダー**：
  - OpenAI
  - Anthropic
  - Google Gemini
  - Deepseek
  - Ollama（ローカルモデル）
  - Fireworks

## アーキテクチャの特徴

### モジュラー設計
- 各LLMプロバイダーは独立したモジュールとして実装
- 新しいプロバイダーの追加が容易
- 統一されたインターフェースで異なるAPIを抽象化

### リアルタイム比較
- WebSocketを使用したストリーミングレスポンス
- 並行実行による高速な結果取得
- 視覚的に分かりやすい比較UI

### ベンチマーク設定
- YAMLファイルによる柔軟なベンチマーク設定
- カスタムプロンプトと評価基準の定義
- 結果の自動保存とレポート生成

## 開発環境の特徴

### Claude Code統合
- カスタムコマンド（`/prime`）でプロジェクトコンテキストを即座に読み込み
- 事前設定された権限で効率的な開発
- プロジェクト固有の設定とワークフロー

### マルチエージェント開発
- Git worktreeを活用した並列開発手法
- 複数のAIエージェントで異なる実装を同時に試行
- 最適な実装の選択とマージが可能

## ユースケース

1. **LLMの選定**：特定のタスクに最適なモデルの選択
2. **コスト最適化**：パフォーマンスと価格のバランス分析
3. **開発効率化**：リアルタイムでのプロンプトエンジニアリング
4. **研究開発**：新しいモデルの性能評価と比較

## 今後の展望

BENCHYは、AIモデルの急速な進化に対応し、開発者が最適なLLMを選択・活用できるようサポートすることを目指しています。継続的な機能追加とモデル対応により、より包括的なベンチマークプラットフォームへと進化していく予定です。